<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>F2-NeRF</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories</h1>
          <h1 class="title is-4 publication-conference"> CVPR 2023 (highlight) </h1>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="https://nearlyemptystring.com/about">Peng Wang</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://liuyuan-pal.github.io/">Yuan Liu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://frozenburning.github.io/">Zhaoxi Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://lingjie0206.github.io/">Lingjie Liu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.hku.hk/index.php/people/academic-staff/taku">Taku Komura</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Wenping Wang</a><sup>4</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>2</sup>S-Lab, Nanyang Technological University,</span>
            <span class="author-block"><sup>3</sup>Max Planck Institute for Informatics</span>
            <span class="author-block"><sup>4</sup>Texas A&M University</span>
            <span class="author-block"><sup>*</sup>Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2303.15951.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.15951"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/totoro97/f2-nerf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://www.dropbox.com/sh/jmfao2c4dp9usji/AAC7Ydj6rrrhy1-VvlAVjyE_a?dl=0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="comparison" controls autoplay muted loop control height width="100%">
        <source src="./static/videos/teaser.mp4"
                      type="video/mp4">
      </video>
      <div class="content has-text-centered">
          <p>
            @Shifeng Park. Trained for ~47 minutes on a single Nvidia GPU.
          </p>
        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF) for novel view synthesis, which enables arbitrary input camera trajectories and only costs a few minutes for training. Existing fast grid-based NeRF training frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed for bounded scenes and rely on space warping to handle unbounded scenes. Existing two widely-used space-warping methods are only designed for the forward-facing trajectory or the 360 degree object-centric trajectory but cannot process arbitrary trajectories. In this paper, we delve deep into the mechanism of space warping to handle unbounded scenes. Based on our analysis, we further propose a novel space-warping method called perspective warping, which allows us to handle arbitrary trajectories in the grid-based NeRF framework. Extensive experiments demonstrate that F2-NeRF is able to use the same perspective warping to render high-quality images on two standard datasets and a new free trajectory dataset collected by us.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">

  <div class="container is-max-desktop">
    <!--/ Paper arch. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Framework</h2>
        <img src='./static/images/pipeline.png'></img>
        <div class="content has-text-justified">
          <p>Pipeline of F2-NeRF. (a) Given a large region of interest, we subdivide the space according to the input view frustums. (b) For each sub-region, we construct a perspective warping function based on the visible cameras. The densities and colors are decoded from the scene feature vectors fetched from the same hash table (d) but using different hash functions (c). See the paper for more details. </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Comparisons</h2>
        <div class="columns is-centered">
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/comparison.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">More results</h2>
        <div class="content is-centered has-text-centered">
          <p>F2-NeRF can be trained using trajectories with different patterns.</p>
        </div>
        <div class="columns is-centered">
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/result_4.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/result_1.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/result_3.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/result_2.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/result_5.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/result_6.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/result_7.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="column content">
            <video id="comparison" controls muted loop control height width="100%">
              <source src="./static/videos/result_8.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Acknowledgement</h2>
        <div class="content has-text-justified">
          <p>
            This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP20221-0012), NTU NAP, and under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). Lingjie Liu and Christian Theobalt have been supported by the ERC Consolidator Grant 4DReply (770784). Peng Wang is supported by Hong Kong PhD Fellowship Scheme.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2023f2nerf,
  title={F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories},
  author={Wang, Peng and Liu, Yuan and Chen, Zhaoxi and Liu, Lingjie and Liu, Ziwei and Komura, Taku and Theobalt, Christian and Wang, Wenping},
  journal={CVPR},
  year={2023}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The source code of the website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
